{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in /Users/xingmingxu/anaconda3/lib/python3.11/site-packages (1.24.3)\n",
      "Requirement already satisfied: scipy in /Users/xingmingxu/anaconda3/lib/python3.11/site-packages (1.10.1)\n",
      "Requirement already satisfied: numpy<1.27.0,>=1.19.5 in /Users/xingmingxu/anaconda3/lib/python3.11/site-packages (from scipy) (1.24.3)\n",
      "Requirement already satisfied: matplotlib in /Users/xingmingxu/anaconda3/lib/python3.11/site-packages (3.7.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Users/xingmingxu/anaconda3/lib/python3.11/site-packages (from matplotlib) (1.0.5)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/xingmingxu/anaconda3/lib/python3.11/site-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/xingmingxu/anaconda3/lib/python3.11/site-packages (from matplotlib) (4.25.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /Users/xingmingxu/anaconda3/lib/python3.11/site-packages (from matplotlib) (1.4.4)\n",
      "Requirement already satisfied: numpy>=1.20 in /Users/xingmingxu/anaconda3/lib/python3.11/site-packages (from matplotlib) (1.24.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/xingmingxu/anaconda3/lib/python3.11/site-packages (from matplotlib) (23.0)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /Users/xingmingxu/anaconda3/lib/python3.11/site-packages (from matplotlib) (9.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /Users/xingmingxu/anaconda3/lib/python3.11/site-packages (from matplotlib) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Users/xingmingxu/anaconda3/lib/python3.11/site-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in /Users/xingmingxu/anaconda3/lib/python3.11/site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Collecting sklearn\n",
      "  Using cached sklearn-0.0.post12.tar.gz (2.6 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25lerror\n",
      "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
      "  \n",
      "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py egg_info\u001b[0m did not run successfully.\n",
      "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
      "  \u001b[31m╰─>\u001b[0m \u001b[31m[15 lines of output]\u001b[0m\n",
      "  \u001b[31m   \u001b[0m The 'sklearn' PyPI package is deprecated, use 'scikit-learn'\n",
      "  \u001b[31m   \u001b[0m rather than 'sklearn' for pip commands.\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m Here is how to fix this error in the main use cases:\n",
      "  \u001b[31m   \u001b[0m - use 'pip install scikit-learn' rather than 'pip install sklearn'\n",
      "  \u001b[31m   \u001b[0m - replace 'sklearn' by 'scikit-learn' in your pip requirements files\n",
      "  \u001b[31m   \u001b[0m   (requirements.txt, setup.py, setup.cfg, Pipfile, etc ...)\n",
      "  \u001b[31m   \u001b[0m - if the 'sklearn' package is used by one of your dependencies,\n",
      "  \u001b[31m   \u001b[0m   it would be great if you take some time to track which package uses\n",
      "  \u001b[31m   \u001b[0m   'sklearn' instead of 'scikit-learn' and report it to their issue tracker\n",
      "  \u001b[31m   \u001b[0m - as a last resort, set the environment variable\n",
      "  \u001b[31m   \u001b[0m   SKLEARN_ALLOW_DEPRECATED_SKLEARN_PACKAGE_INSTALL=True to avoid this error\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m More information is available at\n",
      "  \u001b[31m   \u001b[0m https://github.com/scikit-learn/sklearn-pypi-package\n",
      "  \u001b[31m   \u001b[0m \u001b[31m[end of output]\u001b[0m\n",
      "  \n",
      "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "\u001b[?25h\u001b[1;31merror\u001b[0m: \u001b[1mmetadata-generation-failed\u001b[0m\n",
      "\n",
      "\u001b[31m×\u001b[0m Encountered error while generating package metadata.\n",
      "\u001b[31m╰─>\u001b[0m See above for output.\n",
      "\n",
      "\u001b[1;35mnote\u001b[0m: This is an issue with the package mentioned above, not pip.\n",
      "\u001b[1;36mhint\u001b[0m: See above for details.\n"
     ]
    }
   ],
   "source": [
    "! pip install numpy\n",
    "! pip install scipy\n",
    "! pip install matplotlib\n",
    "! pip install sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# For loss function:\n",
    "def Sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def Sigmoid_derivative(x):\n",
    "        return x * (1 - x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "def tokenize(sentence: str):\n",
    "    \"\"\"\n",
    "    In theory, tokenize(sentence) would also:\n",
    "    This implementation does not take that into account yet.\n",
    "    \"\"\"\n",
    "    return sentence.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# Functions\n",
    "\n",
    "def activation(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "def softmax(x):\n",
    "    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return e_x / e_x.sum()\n",
    "\n",
    "def output(x):\n",
    "    \"\"\"Uses softmax.\"\"\"\n",
    "    return softmax(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "\n",
    "    \"\"\"\n",
    "    A neural network with one hidden layer.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, output_size,\n",
    "                 input_weights=None, hidden_weights=None, \n",
    "                 input_bias=None, hidden_bias=None, output_bias=None):\n",
    "\n",
    "        \"\"\"\n",
    "        Should store:\n",
    "            - array of inputs H_{t}\n",
    "            - array of weights W_{t} (going towards H_{t+1})\n",
    "        \"\"\"\n",
    "\n",
    "        self.loss_value = 0\n",
    "\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "\n",
    "        # Initialize with random weights.\n",
    "        self.input_weights = input_weights if input_weights is not None else np.random.rand(self.input_size, self.hidden_size)\n",
    "        self.hidden_weights = hidden_weights if hidden_weights is not None else np.random.rand(self.hidden_size,output_size)\n",
    "        \n",
    "        self.input_bias = input_bias #if input_bias is not None else np.random.rand(num_points, self.input_size)\n",
    "        self.hidden_bias = hidden_bias #if hidden_bias is not None else np.random.rand(num_points, self.hidden_size)\n",
    "        self.output_bias = output_bias #if output_bias is not None else np.random(num_points, self.output_size)\n",
    "\n",
    "    def show_neural_network(self) -> None:\n",
    "        print(self.input_weights)\n",
    "        print(self.hidden_weights)\n",
    "\n",
    "    def activate(self, x):\n",
    "        return Sigmoid(x)\n",
    "    \n",
    "    def deriv_act(self, x):\n",
    "        return Sigmoid_derivative(x)\n",
    "    \n",
    "    def loss(self, y_true, y_calc):\n",
    "        return np.divide(np.sum(np.square(y_true - y_calc)), 2)\n",
    "\n",
    "    def deriv_loss(self, y_true, y_calc):\n",
    "        return y_calc - y_true\n",
    "\n",
    "    def forward(self, data) -> None:\n",
    "        # Update every bias.\n",
    "        self.input_bias = data\n",
    "        hidden_z = np.matmul(self.input_weights, self.input_bias)\n",
    "        #print(\"hidden preac:\\n\", hidden_z)\n",
    "        self.hidden_bias = self.activate(hidden_z)\n",
    "        output_z = np.matmul(self.hidden_weights, self.hidden_bias)\n",
    "        #print(\"output preac:\\n\", output_z)\n",
    "        self.output_bias = self.activate(output_z)\n",
    "        return self.output_bias\n",
    "        \n",
    "    def backward(self, y_true, learning_rate):\n",
    "\n",
    "        # For T in T -> 1:\n",
    "        # Derive d(L, Wi), d(L, H{i-1})\n",
    "\n",
    "        #print(np.array([self.deriv_act(i) for i in self.output_bias])) #* self.deriv_loss(y_true, self.output_bias)\n",
    "        #print(self.deriv_loss(y_true, self.output_bias))\n",
    "        dsim_output = np.array([self.deriv_act(i) for i in self.output_bias]) * self.deriv_loss(y_true, self.output_bias)\n",
    "        #print(dsim_output)\n",
    "        #print(self.hidden_bias.shape)\n",
    "        dsim_hidden_weights = np.matmul(dsim_output, np.transpose(self.hidden_bias))\n",
    "        dsim_hidden_bias = np.matmul(np.transpose(self.hidden_weights), dsim_output)\n",
    "        #print(dsim_hidden_weights)\n",
    "        #print(dsim_hidden_bias)\n",
    "\n",
    "        dsim_hidden = np.array([self.deriv_act(i) for i in self.hidden_bias]) * dsim_hidden_bias\n",
    "        dsim_input_weights = np.matmul(dsim_hidden, np.transpose(self.input_bias))\n",
    "        dsim_input_bias = np.matmul(np.transpose(self.input_weights), dsim_hidden)\n",
    "\n",
    "        #print(dsim_input_weights)\n",
    "        #print(dsim_input_bias)\n",
    "        \n",
    "        self.hidden_weights -= learning_rate * dsim_hidden_weights\n",
    "        self.input_weights -= learning_rate * dsim_input_weights\n",
    "\n",
    "        return 0\n",
    "    \n",
    "\n",
    "    def train_neural_network(self, train_x, train_y, learning_rate = 0.01, num_epochs=1):\n",
    "\n",
    "        \"\"\"\n",
    "        Trains the neural network on a dataset.\n",
    "        \"\"\"\n",
    "        result = 0\n",
    "        for epochs in range(num_epochs):\n",
    "            fore = self.forward(train_x)\n",
    "            result = fore\n",
    "            self.backward(learning_rate=learning_rate, y_true=train_y)\n",
    "            self.loss_value = self.loss(fore, train_y)\n",
    "            print(self.loss_value)\n",
    "\n",
    "        return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "class RecurrentNNet:\n",
    "\n",
    "    # Should also allow a bias\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, output_size, vector_length, \n",
    "                 bias_h=np.array([[-1.2],[0.8],[-0.8],[0.7]]),\n",
    "                 bias_y=np.array([[1],[-1],[0.5]])):\n",
    "\n",
    "        # h_0 -> h_t -> h_time\n",
    "        self.time = vector_length\n",
    "        self.h_init = np.array([[0.1],[0.1],[0.1],[0.1]])\n",
    "        #np.random.randn(hidden_size, 1) * 0.001\n",
    "\n",
    "        # Create nodes of input_size, (standard) hidden_size, output_size\n",
    "        self.weight_x = np.array([[1,2,1,2,2],\n",
    "                                  [2,1,4,3,-1],\n",
    "                                  [3,3,5,1,0],\n",
    "                                  [4,5,6,2,5]])\n",
    "        # np.random.randn(hidden_size, input_size) # Gets passed through for nnet\n",
    "        self.weight_y = np.array([[1,2,1,2],\n",
    "                                       [2,1,4,3],\n",
    "                                       [3,3,5,1]])\n",
    "        #np.random.randn(hidden_size, hidden_size)\n",
    "        self.weight_hidden = np.array([[0,1,4,5],\n",
    "                                  [-1,2,3,4],\n",
    "                                  [1,3,2,1],\n",
    "                                  [2,4,1,2]])\n",
    "        #np.random.randn(output_size, hidden_size) # Gets passed through for nnet\n",
    "        \n",
    "        self.bh = bias_h # np.zeros((hidden_size, 1))  # hidden bias\n",
    "        self.by = bias_y # np.zeros((output_size, 1))  # output bias\n",
    "\n",
    "    def forward(self, x_data, y_data):\n",
    "\n",
    "        \"\"\"\n",
    "        x_data: [x_1, ..., x_t] where x_i is a token.\n",
    "        y_data: [y_1, ..., y_t] where y_i is a tuple.\n",
    "        \"\"\"\n",
    "\n",
    "        #assert(len(x_data[0]) == len(y_data[0]))\n",
    "        \n",
    "        self.hs = []\n",
    "        self.ys = []\n",
    "        self.d_ksi = []\n",
    "        self.d_hidden = [0] * self.time\n",
    "\n",
    "        loss = 0\n",
    "\n",
    "        prev = None\n",
    "        for i in range(self.time):\n",
    "            if prev is None:\n",
    "                prev = self.h_init\n",
    "\n",
    "            #node.show()\n",
    "            #print(x_data[i])\n",
    "            #node.show()\n",
    "\n",
    "            #print(\"prev\", prev)\n",
    "            #print(\"prev\", prev.shape)\n",
    "            #a = np.matmul(self.weight_hidden, prev)\n",
    "            #print(\"a\", a.shape)\n",
    "            #print(node.weight_x.shape)\n",
    "            #print(x_data[i].shape)\n",
    "            #print(x_data[i])\n",
    "            #print(self.weight_x)\n",
    "            #b = np.matmul(self.weight_x, np.array([x_data[i]]).T)#+ self.bh\n",
    "            #print(\"b\", b.shape)\n",
    "\n",
    "            z_t = np.matmul(self.weight_hidden, prev) +  np.matmul(self.weight_x, np.array([x_data[i]]).T) + self.bh\n",
    "\n",
    "            #print(self.bh.shape)\n",
    "            #print(z_t.shape)\n",
    "            h_t = activation(z_t)\n",
    "\n",
    "            #print(i)\n",
    "            #print(h_t)\n",
    "            #print(h_t.shape)\n",
    "\n",
    "            ksi_t = np.matmul(self.weight_y, h_t) + self.by\n",
    "            y_t = output(ksi_t)\n",
    "            #print(y_t)\n",
    "\n",
    "\n",
    "            self.hs.append(h_t)\n",
    "            self.ys.append(y_t)\n",
    "\n",
    "            #print(np.array([y_data[i]]).T.shape)\n",
    "            #print(y_t.shape)\n",
    "            d_ksi_t = y_t - np.array([y_data[i]]).T\n",
    "            self.d_ksi.append(d_ksi_t)\n",
    "            #print(\"shape\", d_hidden_t.shape)\n",
    "\n",
    "            prev = h_t\n",
    "\n",
    "            loss -= np.dot(y_data[i], (ksi_t - math.log(sum([math.exp(ks) for ks in ksi_t]))))\n",
    "            \n",
    "        print(loss)\n",
    "        print(self.d_ksi)\n",
    "\n",
    "\n",
    "    def backward(self, x_data, y_data):\n",
    "        d_hidden_t = np.matmul(self.weight_y.T, self.d_ksi[self.time-1])\n",
    "        self.d_hidden.append(d_hidden_t)\n",
    "        \n",
    "        d_wy, d_by, d_wh, d_bh, d_wx = np.zeros((3, 4)), np.zeros((3, 1)), np.zeros((4, 4)), np.zeros((4, 1)), np.zeros((4, 5))\n",
    "        for i in range(self.time - 1, 0, -1):\n",
    "            jacobian = np.ones((4, 1)) - self.hs[i] ** 2 # Change hardcoded later\n",
    "            self.d_hidden[i] = np.matmul(self.weight_hidden.T, np.multiply(self.d_hidden[i+1], jacobian)) + \\\n",
    "                np.matmul(self.weight_y.T, self.d_ksi[i])\n",
    "            #print(\"ht\", self.d_hidden[i])\n",
    "            d_wy += np.matmul(self.d_ksi[i], self.d_hidden[i].T)\n",
    "            \n",
    "            d_by += self.d_ksi[i]\n",
    "\n",
    "            #print(jacobian.shape, self.d_hidden[i].shape, self.h_init.shape)\n",
    "            #print(\"result\", np.multiply(jacobian, self.d_hidden[i]))\n",
    "\n",
    "            if i == 0:\n",
    "                a = np.matmul(np.multiply(jacobian, self.d_hidden[i]), self.h_init.T)\n",
    "                #print(a.shape)\n",
    "                d_wh += a\n",
    "            else:\n",
    "                b = np.matmul(np.multiply(jacobian, self.d_hidden[i]), self.hs[i-1].T)\n",
    "                #print(b.shape)\n",
    "                d_wh += b\n",
    "                \n",
    "            d_bh += np.multiply(jacobian, self.d_hidden[i])\n",
    "            #print(np.multiply(jacobian, self.d_hidden[i]).shape)\n",
    "            #print(np.array([x_data[i]]).shape)\n",
    "            d_wx += np.matmul(np.multiply(jacobian, self.d_hidden[i]), np.array([x_data[i]]))\n",
    "\n",
    "        print(d_wh)\n",
    "\n",
    "               \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 3)\n",
      "[8.80888043]\n",
      "[array([[-0.99470748],\n",
      "       [ 0.03331418],\n",
      "       [ 0.9613933 ]]), array([[ 0.0039513 ],\n",
      "       [-0.97080359],\n",
      "       [ 0.96685229]]), array([[ 0.0039513 ],\n",
      "       [ 0.02919641],\n",
      "       [-0.03314771]])]\n",
      "[[ 4.00499518e-10  4.50977775e-10  4.45080052e-10  4.52479953e-10]\n",
      " [ 7.21827487e-11  8.51899845e-11  8.36702595e-11  8.55770656e-11]\n",
      " [ 7.56794754e-09  8.55087173e-09  8.43602991e-09  8.58012248e-09]\n",
      " [-1.98077898e-14 -2.22985697e-14 -2.20075547e-14 -2.23726926e-14]]\n"
     ]
    }
   ],
   "source": [
    "temp = RecurrentNNet(5, 4, 3, 3)\n",
    "\n",
    "\n",
    "x_data=np.array([[0.1, 0.2, 0.4],\n",
    "        [0.2, 0.1, 0.6],\n",
    "        [0.3, 0.5, 0.1],\n",
    "        [0.1, 0.5, 0.5],\n",
    "        [0.3, 0.4, 0.7]])\n",
    "y_data=np.array(\n",
    "    [[1,0,0],\n",
    "        [0,1,0],\n",
    "        [0,0,1]])\n",
    "print(x_data.shape)\n",
    "\n",
    "temp.forward(x_data=x_data.T,\n",
    "             y_data=y_data.T)\n",
    "temp.backward(x_data=x_data.T,\n",
    "              y_data=y_data.T)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
